Choose a scenario:

Calory Tracker
Bug Hunt
Ride Sharing
Medicine
3

Ride Sharing scenario selected.

Here is the scenario description:

You're assigned to optimize and test the matching system for peak-hour ride requests. The current solution loops over all drivers for every rider.

What would be the time complexity of this approach, why is it inefficient during high traffic, and how can you improve it?

what does the code look like

Here is the code:

import time
import random

drivers = [{'id': i, 'location': (random.randint(0, 100), random.randint(0, 100))} for i in range(1000)]
riders = [{'id': j, 'location': (random.randint(0, 100), random.randint(0, 100))} for j in range(1000)]

def distance(a, b):
    return ((a[0] - b[0])**2 + (a[1] - b[1])**2)**0.5

def match_riders_to_drivers(riders, drivers):
    matches = []
    used_drivers = set()
    for rider in riders:
        closest_driver = None
        min_dist = float('inf')
        for driver in drivers:
            if driver['id'] in used_drivers:
                continue
            dist = distance(rider['location'], driver['location'])
            if dist < min_dist:
                min_dist = dist
                closest_driver = driver
        if closest_driver:
            matches.append((rider['id'], closest_driver['id']))
            used_drivers.add(closest_driver['id'])
        else:
            matches.append((rider['id'], None))
    return matches

start = time.time()
matches = match_riders_to_drivers(riders, drivers)
end = time.time()
print(f"Matched {len(matches)} riders in {end - start:.2f} seconds.")

Next Steps: Now that we have the code in front of us, let's dive deeper into the matching algorithm.

Notice how the match_riders_to_drivers function iterates over all drivers for each rider. Think about the number of operations performed when we have n drivers and m riders. Can you estimate the time complexity of this approach?

estimate the time complexity

Let's break down the number of operations performed by the algorithm. For each of the m riders, we're iterating over all n drivers to find the closest match. Think of it as n operations for each of the m riders. Can you express the total number of operations as n * m or, if n â‰ˆ m, an O(n^2) time complexity?

Next Steps: You've correctly estimated the time complexity of this approach. Now, let's think about the implications of this O(n^2) algorithm in a high-traffic scenario. Imagine thousands of riders requesting rides simultaneously. How would this algorithm perform under such a load? Can you think of a way to avoid this performance bottleneck?

for a ride found a first driver avaiable.

As a Junior Developer in the "Optimize the Ride-Sharing Algorithm" scenario, you're right to think about the first available driver. However, in a high-traffic scenario with thousands of riders, looping through all drivers for each rider is costly. Can we index drivers by proximity to quickly find nearby matches? Think about spatial hashing or k-d trees to optimize the lookup process.

index drivers for proximity

What spatial data structure would allow us to efficiently index drivers by proximity? Think about how we can partition the 2D space to facilitate nearest-neighbor searches. Are there any specific considerations for balancing the tree or handling insertion operations?

Next Steps: Great job on considering indexing drivers by proximity! Now, let's think about why this approach is inefficient during high-traffic times. When thousands of ride requests come in, how do you think the current algorithm will handle the load? Will the nested loops and scaling issues lead to a performance bottleneck? Think in terms of scale: what happens when the number of requests and drivers grows exponentially?